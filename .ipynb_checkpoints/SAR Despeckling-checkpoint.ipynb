{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z2IU0LQd7oHZ"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "# import natsort\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict \n",
    "import datetime\n",
    "from scipy import special\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /home/201751066/.local/lib/python3.6/site-packages (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib64/python3.6/site-packages (from scipy) (1.19.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uw971ve72tuX",
    "outputId": "e153d439-8a52-4a86-99dc-65588b21bebb"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive' )\n",
    "# drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "i8wvUB2k1gSk",
    "outputId": "7b84a7cd-8ed3-4aba-8de9-99e63f9b9c75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makshi11\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sntkwH_U7rbd"
   },
   "outputs": [],
   "source": [
    "class NoisyImageDataset(Dataset):\n",
    "    def __init__(self, main_dir, transform):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        all_imgs = []\n",
    "        for f in os.listdir(main_dir):\n",
    "            if not f.startswith('.'):\n",
    "                all_imgs.append(f)\n",
    "        self.total_imgs = all_imgs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
    "        image = Image.open(img_loc).convert(\"LA\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return torch.reshape(tensor_image[0], (1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZikIjb66cB5B"
   },
   "outputs": [],
   "source": [
    "def load_data(images_dir_list, L, p=1, batch_size=10):\n",
    "  # images_dir = 'gdrive/My Drive/ImageNet/noisy/'\n",
    "  # plants_dir = images_dir + 'plants_L_'\n",
    "  # animals_dir = images_dir + 'animals_L_'\n",
    "  # scenery_dir = images_dir + 'scenery_L_'\n",
    "\n",
    "  data_transforms = transforms.Compose([\n",
    "                                        transforms.RandomResizedCrop(256),\n",
    "                                        transforms.ToTensor()])\n",
    "                                                            \n",
    "  data_set_list = []\n",
    "  for li in images_dir_list:\n",
    "    s = ''\n",
    "    if L !=0:\n",
    "      s = str(L)\n",
    "    data_set_list.append(NoisyImageDataset(li+s, transform=data_transforms))\n",
    "    data_set_list.append(NoisyImageDataset(li+s, transform=data_transforms))\n",
    "    data_set_list.append(NoisyImageDataset(li+s, transform=data_transforms))\n",
    "  data_set = torch.utils.data.ConcatDataset(data_set_list)\n",
    "  l = int(len(data_set)*p)\n",
    "  print(l)\n",
    "  train, valid = torch.utils.data.random_split(data_set, [l, len(data_set)-l])\n",
    "  trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "  validloader = []\n",
    "  if len(valid) != 0:\n",
    "    validloader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "  return trainloader, validloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kPJZDFU8b2c"
   },
   "source": [
    "# Network Architecture and other necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5KQzXYyH-pNO"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "  def __init__(self, in_channels=1, out_channels=1, init_features=32):\n",
    "    super(UNet, self).__init__()\n",
    "\n",
    "    features = init_features\n",
    "    self.encoder1 = UNet._block(in_channels, features, name=\"enc1\", kernel_size=7, padding=3)\n",
    "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    self.encoder2 = UNet._block(features, features * 2, name=\"enc2\", kernel_size=5, padding=2)\n",
    "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "    self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "    self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "    self.upconv4 = nn.ConvTranspose2d(\n",
    "        features * 16, features * 8, kernel_size=2, stride=2\n",
    "    )\n",
    "    self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "    self.upconv3 = nn.ConvTranspose2d(\n",
    "        features * 8, features * 4, kernel_size=2, stride=2\n",
    "    )\n",
    "    self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "    self.upconv2 = nn.ConvTranspose2d(\n",
    "        features * 4, features * 2, kernel_size=2, stride=2\n",
    "    )\n",
    "    self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "    self.upconv1 = nn.ConvTranspose2d(\n",
    "        features * 2, features, kernel_size=2, stride=2\n",
    "    )\n",
    "    self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "    self.conv = nn.Conv2d(\n",
    "        in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    enc1 = self.encoder1(x)\n",
    "#     print(enc1.size())\n",
    "    enc2 = self.encoder2(self.pool1(enc1))\n",
    "#     print(enc2.size())\n",
    "    enc3 = self.encoder3(self.pool2(enc2))\n",
    "#     print(enc3.size())\n",
    "    enc4 = self.encoder4(self.pool3(enc3))\n",
    "#     print(enc4.size())\n",
    "\n",
    "    bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "#     print(bottleneck.size())\n",
    "    dec4 = self.upconv4(bottleneck)\n",
    "#     print(dec4.size())\n",
    "    dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "    dec4 = self.decoder4(dec4)\n",
    "    dec3 = self.upconv3(dec4)\n",
    "#     print(dec3.size())\n",
    "    dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "    dec3 = self.decoder3(dec3)\n",
    "    dec2 = self.upconv2(dec3)\n",
    "#     print(dec2.size())\n",
    "    dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "    dec2 = self.decoder2(dec2)\n",
    "    dec1 = self.upconv1(dec2)\n",
    "#     print(dec1.size())\n",
    "    dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "    dec1 = self.decoder1(dec1)\n",
    "    out = torch.sigmoid(self.conv(dec1))\n",
    "#     print(out.size())\n",
    "    return out\n",
    "\n",
    "  @staticmethod\n",
    "  def _block(in_channels, features, name, kernel_size=3, padding=1):\n",
    "    return nn.Sequential(\n",
    "        OrderedDict(\n",
    "            [\n",
    "                (\n",
    "                    name + \"conv1\",\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=features,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=padding,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                ),\n",
    "                (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                (\n",
    "                    name + \"conv2\",\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=features,\n",
    "                        out_channels=features,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=padding,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                ),\n",
    "                (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "  def __init__(self, in_channels=1, out_channels=1, init_features=64):\n",
    "    super(UNet, self).__init__()\n",
    "\n",
    "    features = init_features\n",
    "    self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
    "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "    self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "    self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "    self.upconv4 = nn.ConvTranspose2d(\n",
    "        features * 16, features * 8, kernel_size=2, stride=2\n",
    "    )\n",
    "    self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "    self.upconv3 = nn.ConvTranspose2d(\n",
    "        features * 8, features * 4, kernel_size=2, stride=2\n",
    "    )\n",
    "    self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "    self.upconv2 = nn.ConvTranspose2d(\n",
    "        features * 4, features * 2, kernel_size=2, stride=2\n",
    "    )\n",
    "    self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "    self.upconv1 = nn.ConvTranspose2d(\n",
    "        features * 2, features, kernel_size=2, stride=2\n",
    "    )\n",
    "    self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "    self.conv = nn.Conv2d(\n",
    "        in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    enc1 = self.encoder1(x)\n",
    "#     print(enc1.size())\n",
    "    enc2 = self.encoder2(self.pool1(enc1))\n",
    "#     print(enc2.size())\n",
    "    enc3 = self.encoder3(self.pool2(enc2))\n",
    "#     print(enc3.size())\n",
    "    enc4 = self.encoder4(self.pool3(enc3))\n",
    "#     print(enc4.size())\n",
    "\n",
    "    bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "#     print(bottleneck.size())\n",
    "    dec4 = self.upconv4(bottleneck)\n",
    "#     print(dec4.size())\n",
    "    dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "    dec4 = self.decoder4(dec4)\n",
    "    dec3 = self.upconv3(dec4)\n",
    "#     print(dec3.size())\n",
    "    dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "    dec3 = self.decoder3(dec3)\n",
    "    dec2 = self.upconv2(dec3)\n",
    "#     print(dec2.size())\n",
    "    dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "    dec2 = self.decoder2(dec2)\n",
    "    dec1 = self.upconv1(dec2)\n",
    "#     print(dec1.size())\n",
    "    dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "    dec1 = self.decoder1(dec1)\n",
    "    out = torch.sigmoid(self.conv(dec1))\n",
    "#     print(out.size())\n",
    "    return out\n",
    "\n",
    "  @staticmethod\n",
    "  def _block(in_channels, features, name, kernel_size=3, padding=1):\n",
    "    return nn.Sequential(\n",
    "        OrderedDict(\n",
    "            [\n",
    "                (\n",
    "                    name + \"conv1\",\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=features,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=padding,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                ),\n",
    "                (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                (\n",
    "                    name + \"conv2\",\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=features,\n",
    "                        out_channels=features,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=padding,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                ),\n",
    "                (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "w-fcEdnS9jgu"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "RSxhgO7_hwJ2",
    "outputId": "2e5d2ee6-607f-4ddf-f94f-b54f54a361b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "id": "_SA44BEt1-vS",
    "outputId": "adfebb09-b707-4a2b-c55b-d38c58e74ad6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makshi11\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/201751066/.conda/envs/ssl/lib/python3.8/site-packages/IPython/html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.8<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">major-sunset-241</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/akshi11/ssl\" target=\"_blank\">https://wandb.ai/akshi11/ssl</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/akshi11/ssl/runs/7yp6t5mz\" target=\"_blank\">https://wandb.ai/akshi11/ssl/runs/7yp6t5mz</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20210121_134210-7yp6t5mz</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(7yp6t5mz)</h1><p></p><iframe src=\"https://wandb.ai/akshi11/ssl/runs/7yp6t5mz\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fcf79d48850>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"ssl\")\n",
    "# wandb.login(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpF6HCVy8jpx"
   },
   "source": [
    "# Optimizers, loss functions and training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ41fYqvHNrM"
   },
   "source": [
    "# Train model for SAR Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_speckle(clean_im, L):\n",
    "    M = np.log(256)\n",
    "    m = 0\n",
    "    s = torch.zeros_like(clean_im)\n",
    "    for k in range(0, L):\n",
    "        gamma1 = torch.normal(mean=0, std=1, size=clean_im.size())**2\n",
    "        gamma2 = torch.normal(mean=0, std=1, size=clean_im.size())**2\n",
    "        s = s + torch.abs(gamma1 + gamma2)\n",
    "    s_amplitude = torch.sqrt(s / L)\n",
    "#     print('linear noise',s_amplitude)\n",
    "    log_speckle = torch.log(s_amplitude)\n",
    "#     print('log noise',log_speckle)\n",
    "    log_norm_speckle = log_speckle / (M - m)\n",
    "#     print('normal log noise',log_norm_speckle)\n",
    "#     print('clean',clean_im)\n",
    "    noisy_im = clean_im + log_norm_speckle\n",
    "#     print('noisy',noisy_im)\n",
    "    noisy_im = torch.clamp(noisy_im,min=0.0, max = 1.0)\n",
    "#     print('noisy after clamp',noisy_im)\n",
    "    return noisy_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8B9cjtGEwAiR"
   },
   "outputs": [],
   "source": [
    "dir_list = './qgissenti'\n",
    "data_transforms = transforms.Compose([\n",
    "                                      transforms.ToTensor()])\n",
    "data_set = NoisyImageDataset(dir_list, data_transforms)\n",
    "dataloader = torch.utils.data.DataLoader(data_set, batch_size=len(data_set), shuffle=True, num_workers=1)\n",
    "# num_of_pixels = len(data_set) * 256 * 256\n",
    "# total_sum = 0\n",
    "# for batch in dataloader: \n",
    "# #     print(batch[0].size())\n",
    "#     total_sum += batch[0].sum()\n",
    "# mean = total_sum / num_of_pixels\n",
    "# sum_of_squared_error = 0\n",
    "# for batch in dataloader: \n",
    "#     sum_of_squared_error += ((batch[0] - mean).pow(2)).sum()\n",
    "# std = torch.sqrt(sum_of_squared_error / num_of_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4NDU2chRlwa",
    "outputId": "01a7ec5b-5594-4e49-e88f-72eb5082a062"
   },
   "outputs": [],
   "source": [
    "# data = next(iter(dataloader))\n",
    "# mean, std = data[0].mean(), data[0].std()\n",
    "# print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "                                      transforms.RandomRotation(90),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(0.5, 0.5)\n",
    "                                        ])\n",
    "dataset = NoisyImageDataset(dir_list, data_transforms)\n",
    "# dataloaderSAR = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0exay_P6fAN7"
   },
   "outputs": [],
   "source": [
    "l = int(len(dataset)*0.9)\n",
    "train, valid = torch.utils.data.random_split(dataset, [l, len(dataset)-l])\n",
    "trainloaderSAR = torch.utils.data.DataLoader(train, batch_size=16, shuffle=True, num_workers=2)\n",
    "validloaderSAR = torch.utils.data.DataLoader(valid, batch_size=10, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = './BSD'\n",
    "datatransforms = transforms.Compose([ \n",
    "                                      transforms.RandomResizedCrop(256),\n",
    "                                      transforms.ToTensor()])\n",
    "data_set_train = NoisyImageDataset(dirpath+'/train', datatransforms)\n",
    "data_set_valid = NoisyImageDataset(dirpath+'/valid', datatransforms)\n",
    "dataloadertrain = torch.utils.data.DataLoader(data_set_train, batch_size=1, shuffle=True, num_workers=1)\n",
    "dataloadervalid = torch.utils.data.DataLoader(data_set_valid, batch_size=1, shuffle=True, num_workers=1)\n",
    "for idx,image in enumerate(dataloadertrain):\n",
    "#     b_hat = 10.0\n",
    "#     noise1 = torch.from_numpy(np.random.gamma(b_hat, 1/b_hat, image.shape))\n",
    "#     image = image*noise1\n",
    "    image = lintolog(image)\n",
    "    image = simulate_speckle(image, 4)\n",
    "    plt.imsave('./BSDnoisy/train/'+str(idx)+'.jpg',logtolin(image[0][0]), cmap='gray')\n",
    "for idx,image in enumerate(dataloadervalid):\n",
    "#     b_hat = 10.0\n",
    "#     noise1 = torch.from_numpy(np.random.gamma(b_hat, 1/b_hat, image.shape))\n",
    "#     image = image*noise1\n",
    "    image = lintolog(image)\n",
    "    image = simulate_speckle(image, 4)\n",
    "    plt.imsave('./BSDnoisy/train/val'+str(idx)+'.jpg',logtolin(image[0][0]), cmap='gray')\n",
    "#     image = lintolog(image)\n",
    "#     img = simulate_speckle(image, 10)\n",
    "#     plt.imsave('./BSDcorr/n2/valid/'+str(idx)+'.jpg',logtolin(img[0][0]), cmap='gray')   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_transforms = transforms.Compose([\n",
    "                                      transforms.RandomRotation(90),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(0.5, 0.5)\n",
    "                                        ])\n",
    "# validset = NoisyImageDataset(dirpath+'/valid', data_transforms)\n",
    "dataset = NoisyImageDataset('BSDnoisy/train', data_transforms)\n",
    "l = int(len(dataset)*0.8)\n",
    "train, valid = torch.utils.data.random_split(dataset, [l, len(dataset)-l])\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=16, shuffle=True, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(valid, batch_size=16, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "SDjXMAmZ4Szn",
    "outputId": "f16aae4a-66a1-4d55-e818-9ab6a070a75e"
   },
   "outputs": [],
   "source": [
    "data = next(iter(dataloaderSAR))\n",
    "mean_new, std_new = data[0].mean(), data[0].std()\n",
    "print(mean_new, std_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YuxXrDcuIKJP",
    "outputId": "fbca364a-3859-4e1e-d887-b26d9cbb4b1a"
   },
   "outputs": [],
   "source": [
    "model_S = UNet().double()\n",
    "if use_cuda:\n",
    "  model_S = model_S.cuda()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSbI2EjHElim"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "  if isinstance(m, nn.Conv2d):\n",
    "    nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "  elif isinstance(m, nn.BatchNorm2d):\n",
    "    nn.init.constant_(m.weight, 1)\n",
    "    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "  elif isinstance(m, nn.Linear):\n",
    "    nn.init.kaiming_normal_(m.weight)\n",
    "    nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOKBGst_EshQ",
    "outputId": "1c9eb1a8-cf49-4057-b3a8-a6d8f80cd527"
   },
   "outputs": [],
   "source": [
    "model_SAR = model_S.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmHMg2a23Ygs",
    "outputId": "ced4a000-dde3-4e0e-9f89-8c9e58cbc9c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7f60ba0a2390>]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(model_SAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "aCvxRwvjwsEC"
   },
   "outputs": [],
   "source": [
    "class SURE_Loss(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, y, y_hat, z_hat, n, x_dim, var, batch_size, eps):\n",
    "    div = (1/eps)*torch.sum((n * (z_hat - y_hat)))   #.view(-1,y.shape[0]*y.shape[1] )\n",
    "    sure = (1.0 / batch_size)*(torch.sum((y - y_hat)**2) - (batch_size * x_dim * x_dim * var) + (2*var*div))\n",
    "    return abs(sure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch(image, max_val=1, min_val=0):\n",
    "    image, dtype = to_float(image)\n",
    "    img_min = np.min(image)\n",
    "    img_max = np.max(image)\n",
    "    image_stretched = min_val + (max_val - min_val) * (image - img_min) / (img_max - img_min)\n",
    "    image_stretched = change_type(image_stretched, dtype)\n",
    "    return image_stretched\n",
    "    \n",
    "def change_type(image, dtype):\n",
    "    current_dtype = image.dtype\n",
    "    if current_dtype != dtype:\n",
    "        image = image.astype(dtype)\n",
    "    return image\n",
    "\n",
    "def to_float(image):\n",
    "    dtype = image.dtype\n",
    "    if dtype != np.float32:\n",
    "        image = image.astype(np.float32)\n",
    "    return image, dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7s_oMhzlUpTq"
   },
   "outputs": [],
   "source": [
    "def lintolog(image, max_val=1.0, min_val=-1.0):\n",
    "    image = image.numpy()\n",
    "    LIN_MAX = 1.0\n",
    "    LOG_MAX = np.log10(LIN_MAX + 1)\n",
    "    image, dtype = to_float(image)\n",
    "    img_min = np.min(image)\n",
    "    img_max = np.max(image)\n",
    "    stretch_max = LIN_MAX * (img_max - min_val) / (max_val - min_val)\n",
    "    stretch_min = LIN_MAX * (img_min - min_val) / (max_val - min_val)\n",
    "    image = stretch(image, max_val=stretch_max, min_val=stretch_min)\n",
    "    image = np.log10(image + 1)\n",
    "    image = image / LOG_MAX\n",
    "    image = change_type(image, dtype)\n",
    "    return torch.DoubleTensor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logtolin(image, max_val=1.0, min_val=0):\n",
    "    image = image.detach().numpy()\n",
    "    LIN_MAX = 1.0\n",
    "    LOG_MAX = np.log10(LIN_MAX + 1)\n",
    "    image, dtype = to_float(image)\n",
    "    img_min = np.min(image)\n",
    "    img_max = np.max(image)\n",
    "    stretch_max = LOG_MAX * (img_max - min_val) / (max_val - min_val)\n",
    "    stretch_min = LOG_MAX * (img_min - min_val) / (max_val - min_val)\n",
    "    image = stretch(image, max_val=stretch_max, min_val=stretch_min)\n",
    "    image = np.power(10, image) - 1\n",
    "    image = image / LIN_MAX \n",
    "    image = change_type(image, dtype)\n",
    "    return torch.DoubleTensor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MU_2NrTMuihl"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cgsxN45H53lJ"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_SAR.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 30, eta_min=0, last_epoch=-1)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TavZ4SvO3dv5"
   },
   "outputs": [],
   "source": [
    "criterion = SURE_Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0689382278476838\n"
     ]
    }
   ],
   "source": [
    "print(special.polygamma(1,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "rujlgRr8vxAo"
   },
   "outputs": [],
   "source": [
    "def train_SAR(n_epochs, trainloader, model, optimizer, criterion, use_cuda, save_path, lr=0.01):\n",
    "  \"\"\"returns trained model\"\"\"\n",
    "  # n_epochs = 20\n",
    "  # trainloader = dataloader_25\n",
    "  # save_path = 'gdrive/My Drive/Checkpoints_UNet/checkpoint_imnet.pt'\n",
    "  valid_loss_min = np.Inf\n",
    "  train_loss_min = np.Inf\n",
    "  for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    # initialize variables to monitor training and validation loss\n",
    "\n",
    "    # monitor time\n",
    "    current_time = datetime.datetime.now()\n",
    "    print(current_time) \n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    # accuracy = 0.0\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    wandb.log({\"lr\": optimizer.param_groups[0]['lr']})\n",
    "    for batch_idx, image in enumerate(trainloader):\n",
    "      # move to GPU\n",
    "#       print(1.0/torch.var(image))\n",
    "      print(batch_idx, end=\" \")\n",
    "      image = image.double()\n",
    "#       b_hat = 100.0\n",
    "#       noise1 = torch.from_numpy(np.random.gamma(b_hat, 1/b_hat, image.shape))\n",
    "# #       noise2 = torch.from_numpy(np.random.gamma(b_hat, 1/b_hat, image.shape))\n",
    "#       noisy1 = image*noise1\n",
    "# #       noisy2 = image*noise2\n",
    "      image = lintolog(image)\n",
    "#       noisy1 = lintolog(noisy1)\n",
    "#       noisy2 = lintolog(noisy2)\n",
    "#       plt.imshow(lintolog(noise1)[0][0],cmap='gray')\n",
    "#       noisy1 = noisy1 + lintolog(noise1)\n",
    "#       print(max(noisy1[0][0][0]))\n",
    "#       noisy2 = torch.clamp(noisy2 + lintolog(noise2), min=0, max=1)\n",
    "#       plt.imshow(noisy1[0][0],cmap='gray')\n",
    "      if use_cuda:\n",
    "        image = image.cuda()\n",
    "#         noisy1 = noisy1.cuda()\n",
    "#         noisy2 = noisy2.cuda()\n",
    "      ## find the loss and update the model parameters accordingly\n",
    "      ## record the average training loss, using something like\n",
    "      ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "      optimizer.zero_grad()\n",
    "      y_hat = model(image)\n",
    "      # y_hat = forward_NoiseEST(image, x_hat, use_cuda)\n",
    "      # print('y_hat calculated')\n",
    "      # loss = criterion(y_hat, image)\n",
    "      eps = 0.000001\n",
    "      x_dim = 256\n",
    "      n = torch.DoubleTensor(np.random.normal(0, 1, size=image.size()))\n",
    "      img_z = image + torch.DoubleTensor(n*eps).cuda()\n",
    "      z_hat = model(img_z)\n",
    "      sigma = 1/torch.var(image)\n",
    "#       print(sigma)\n",
    "      var = (sigma/255.0)**2\n",
    "      loss = criterion(image, y_hat, z_hat, n.cuda(), x_dim, var, 10, eps)\n",
    "      loss.backward()\n",
    "      # print('loss calculated')\n",
    "      optimizer.step()\n",
    "      # print('backpropagation done')\n",
    "      train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "      wandb.log({\"Running Loss\": loss.data})\n",
    "      \n",
    "    \n",
    "#     scheduler.step()\n",
    "    # model.eval()\n",
    "    # for batch_idx, image in enumerate(validloader):\n",
    "    #   image = image.double()\n",
    "    #   if use_cuda:\n",
    "    #     image = image.cuda()\n",
    "\n",
    "    #   x_hat = model(image)\n",
    "    #   y_hat = forward_NoiseEST(image, x_hat, use_cuda)\n",
    "    #   loss = criterion(y_hat, image)\n",
    "    #   valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "    #   wandb.log({\"Valid Loss\": valid_loss})\n",
    "    print()   \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} '.format(\n",
    "        epoch, \n",
    "        train_loss,\n",
    "        # valid_loss\n",
    "        ))\n",
    "    wandb.log({\"Train Loss\": train_loss})\n",
    "    if train_loss <= train_loss_min:\n",
    "        print('Train loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(train_loss_min, train_loss))\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "#         wandb.save(save_path)\n",
    "        train_loss_min = train_loss\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SAR_N2N_valid(n_epochs, trainloader, validloader, model, optimizer, criterion, use_cuda, save_path, lr=0.0001):\n",
    "  \"\"\"returns trained model\"\"\"\n",
    "  # n_epochs = 20\n",
    "  # trainloader = dataloader_25\n",
    "  # save_path = 'gdrive/My Drive/Checkpoints_UNet/checkpoint_imnet.pt'\n",
    "  valid_loss_min = np.Inf\n",
    "  train_loss_min = np.Inf\n",
    "  for epoch in range(1, n_epochs+1):\n",
    "#     if epoch%10 == 0:\n",
    "#         lr = lr/10.0\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # initialize variables to monitor training and validation loss\n",
    "\n",
    "    # monitor time\n",
    "    current_time = datetime.datetime.now()\n",
    "    print(current_time) \n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    # accuracy = 0.0\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    wandb.log({\"lr\": optimizer.param_groups[0]['lr']})\n",
    "    for batch_idx, image in enumerate(trainloader):\n",
    "      # move to GPU\n",
    "      print(batch_idx, end=\" \")\n",
    "      image = image.double()\n",
    "#       image = lintolog(image)\n",
    "      b_hat = 10.0\n",
    "      noise1 = torch.from_numpy(np.random.gamma(b_hat, 1/b_hat, image.shape))\n",
    "#       noise2 = torch.from_numpy(np.random.gamma(b_hat, 1/b_hat, image.shape))\n",
    "      \n",
    "      y1 = image*noise1\n",
    "#       y2 = image*noise2\n",
    "#       y1 = torch.clamp(y1, max=0.0, min=1.0)\n",
    "#       y2 = torch.clamp(y2, max=0.0, min=1.0)\n",
    "      image = lintolog(image)\n",
    "#       y1 = simulate_speckle(image, 10)\n",
    "#       y2 = simulate_speckle(image, 10)\n",
    "#       print(torch.var(y1), torch.var(y2))\n",
    "#       y1 = image + torch.from_numpy(np.random.normal(0, 0.1, image.shape))\n",
    "#       y2 = image + torch.from_numpy(np.random.normal(0, 0.1, image.shape))\n",
    "      y1 = lintolog(y1)\n",
    "#       y2 = lintolog(y2)\n",
    "#       w = (y1+y2)/2\n",
    "#       v = w+torch.DoubleTensor(np.random.normal(0, torch.var(w), size=w.size()))\n",
    "#       w = (noisy1+noisy2)/2\n",
    "#       print(torch.sqrt(torch.var(noisy2)))\n",
    "#       print(torch.sqrt(torch.var(w)))\n",
    "#       plt.imshow(lintolog(noise1)[0][0],cmap='gray')\n",
    "#       noisy1 = noisy1 + lintolog(noise1)\n",
    "#       print(max(noisy1[0][0][0]))\n",
    "#       noisy2 = torch.clamp(noisy2 + lintolog(noise2), min=0, max=1)\n",
    "#       plt.imshow(noisy1[0][0],cmap='gray')\n",
    "      if use_cuda:\n",
    "#         v = v.cuda()\n",
    "#         w = w.cuda()\n",
    "        image = image.cuda()\n",
    "#         w = w.cuda()\n",
    "        y1 = y1.cuda()\n",
    "#         y2 = y2.cuda()\n",
    "    \n",
    "\n",
    "      ## find the loss and update the model parameters accordingly\n",
    "      ## record the average training loss, using something like\n",
    "      ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "      optimizer.zero_grad()\n",
    "      y_hat = model(y1)\n",
    "      # y_hat = forward_NoiseEST(image, x_hat, use_cuda)\n",
    "      # print('y_hat calculated')\n",
    "      # loss = criterion(y_hat, image)\n",
    "#       print(torch.var(image))\n",
    "#       print(torch.var(noisy1))\n",
    "#       var = 1.64493407/(np.log(256)**2)\n",
    "#       var = 0.0014\n",
    "#       sigma = 10.0\n",
    "#       eps = 1.6*sigma*0.0001\n",
    "#       x_dim = 256\n",
    "#       var = (sigma/255.0)**2\n",
    "# #       print(torch.var(w))\n",
    "#       n = torch.DoubleTensor(np.random.normal(0, 1, size=w.size()))\n",
    "#       neps = torch.DoubleTensor(n*eps).cuda()\n",
    "#       img_z = v + neps\n",
    "#       z_hat = model(img_z)\n",
    "      \n",
    "#       print(sigma)\n",
    "      \n",
    "      loss = criterion(image, y_hat)\n",
    "      loss.backward()\n",
    "      # print('loss calculated')\n",
    "      optimizer.step()\n",
    "      # print('backpropagation done')\n",
    "      train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "      wandb.log({\"Loss\": train_loss})\n",
    "      \n",
    "    \n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    for batch_idx, image in enumerate(validloader):\n",
    "      image = image.double()\n",
    "#       image = lintolog(image)\n",
    "      b_hat = 10.0\n",
    "      noise1 = torch.from_numpy(np.random.gamma(b_hat, 1/b_hat, image.shape))\n",
    "#       noise2 = torch.from_numpy(np.random.gamma(b_hat, 1/b_hat, image.shape))\n",
    "      \n",
    "      y1 = image*noise1\n",
    "      image = lintolog(image)\n",
    "#       y2 = image*noise2\n",
    "#       y1 = torch.clamp(y1, max=0.0, min=1.0)\n",
    "#       y2 = torch.clamp(y2, max=0.0, min=1.0)\n",
    "      y1 = lintolog(y1)\n",
    "#       y2 = lintolog(y2)\n",
    "      if use_cuda:\n",
    "        y1 = y1.cuda()\n",
    "#         y2 = y2.cuda()\n",
    "        image = image.cuda()\n",
    "#         noisy1 = noisy1.cuda()\n",
    "      y_hat = model(y1)\n",
    "#       var = 0.0014\n",
    "#       sigma = 10.0\n",
    "#       eps = 1.6*sigma*0.0001\n",
    "#       var = (sigma/255.0)**2\n",
    "#       x_dim = 256\n",
    "#       n = torch.DoubleTensor(np.random.normal(0, 1, size=w.size()))\n",
    "#       neps = torch.DoubleTensor(n*eps).cuda()\n",
    "#       img_z = v + neps\n",
    "#       z_hat = model(img_z)\n",
    "      loss = criterion(image, y_hat)\n",
    "      valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "      \n",
    "    print()   \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValid Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    wandb.log({\"Train Loss\": train_loss, \n",
    "              \"Valid Loss\": valid_loss})\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Valid loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "#         wandb.save(save_path)\n",
    "        valid_loss_min = valid_loss\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SAR_ESURE_valid(n_epochs, trainloader, validloader, model, optimizer, criterion, use_cuda, save_path, lr=0.0001):\n",
    "  \"\"\"returns trained model\"\"\"\n",
    "  # n_epochs = 20\n",
    "  # trainloader = dataloader_25\n",
    "  # save_path = 'gdrive/My Drive/Checkpoints_UNet/checkpoint_imnet.pt'\n",
    "  valid_loss_min = np.Inf\n",
    "  train_loss_min = np.Inf\n",
    "  for epoch in range(1, n_epochs+1):\n",
    "#     if epoch%10 == 0:\n",
    "#         lr = lr/10.0\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # initialize variables to monitor training and validation loss\n",
    "\n",
    "    # monitor time\n",
    "    current_time = datetime.datetime.now()\n",
    "    print(current_time) \n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    # accuracy = 0.0\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    wandb.log({\"lr\": optimizer.param_groups[0]['lr']})\n",
    "    for batch_idx, image in enumerate(trainloader):\n",
    "      # move to GPU\n",
    "      print(batch_idx, end=\" \")\n",
    "      image = image.double()\n",
    "      image = lintolog(image)\n",
    "#       b_hat = 10.0\n",
    "#       noise1 = torch.from_numpy(np.random.gamma(b_hat, 1/b_hat, image.shape))\n",
    "#       noise2 = torch.from_numpy(np.random.gamma(b_hat, 1/b_hat, image.shape))\n",
    "      \n",
    "#       y1 = image*noise1\n",
    "#       y1 = torch.clamp(y1, max=0.0, min=1.0)\n",
    "#       y2 = image*noise2\n",
    "      \n",
    "#       image = lintolog(image)\n",
    "#       y1 = simulate_speckle(image, 10)\n",
    "#       y2 = simulate_speckle(image, 10)\n",
    "#       print(torch.var(y1), torch.var(y2))\n",
    "#       y1 = image + torch.from_numpy(np.random.normal(0, 0.1, image.shape))\n",
    "#       y2 = image + torch.from_numpy(np.random.normal(0, 0.1, image.shape))\n",
    "#       y1 = lintolog(y1)\n",
    "      y1 = image + torch.from_numpy(np.random.normal(0, 0.1, image.shape))\n",
    "#       y2 = lintolog(y2)\n",
    "#       w = (y1+y2)/2\n",
    "#       v = w+torch.DoubleTensor(np.random.normal(0, torch.var(w), size=w.size()))\n",
    "#       w = (noisy1+noisy2)/2\n",
    "#       print(torch.sqrt(torch.var(noisy2)))\n",
    "#       print(torch.sqrt(torch.var(w)))\n",
    "#       plt.imshow(lintolog(noise1)[0][0],cmap='gray')\n",
    "#       noisy1 = noisy1 + lintolog(noise1)\n",
    "#       print(max(noisy1[0][0][0]))\n",
    "#       noisy2 = torch.clamp(noisy2 + lintolog(noise2), min=0, max=1)\n",
    "#       plt.imshow(noisy1[0][0],cmap='gray')\n",
    "      if use_cuda:\n",
    "#         v = v.cuda()\n",
    "#         w = w.cuda()\n",
    "        image = image.cuda()\n",
    "        y1 = y1.cuda()\n",
    "\n",
    "      ## find the loss and update the model parameters accordingly\n",
    "      ## record the average training loss, using something like\n",
    "      ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "      optimizer.zero_grad()\n",
    "      y_hat = model(y1)\n",
    "      # y_hat = forward_NoiseEST(image, x_hat, use_cuda)\n",
    "      # print('y_hat calculated')\n",
    "      # loss = criterion(y_hat, image)\n",
    "#       print(torch.var(image))\n",
    "#       print(torch.var(noisy1))\n",
    "#       var = 1.64493407/(np.log(256)**2)\n",
    "#       var = 0.0014\n",
    "      sigma = 15.0\n",
    "      eps = 1.6*sigma*0.0001\n",
    "      x_dim = 256\n",
    "      var = (sigma/255.0)**2\n",
    "#       print(torch.var(w))\n",
    "      n = torch.DoubleTensor(np.random.normal(0, 1, size=y1.size()))\n",
    "      neps = torch.DoubleTensor(n*eps).cuda()\n",
    "      img_z = y1 + neps\n",
    "      z_hat = model(img_z)\n",
    "      \n",
    "#       print(sigma)\n",
    "      \n",
    "      loss = criterion(image, y_hat, z_hat, n.cuda(), x_dim, var, 16, eps)\n",
    "      loss.backward()\n",
    "      # print('loss calculated')\n",
    "      optimizer.step()\n",
    "      # print('backpropagation done')\n",
    "      train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "      wandb.log({\"Loss\": train_loss})\n",
    "      \n",
    "    \n",
    "    scheduler.step()\n",
    "#     model.eval()\n",
    "#     for batch_idx, image in enumerate(validloader):\n",
    "#       image = image.double()\n",
    "#       image = lintolog(image)\n",
    "# #       b_hat = 10.0\n",
    "# #       noise1 = torch.from_numpy(np.random.gamma(b_hat, 1/b_hat, image.shape))\n",
    "# #       noise2 = torch.from_numpy(np.random.gamma(b_hat, 1/b_hat, image.shape))\n",
    "# #       y1 = image*noise1\n",
    "# #       y1 = torch.clamp(y1, max=0.0, min=1.0)\n",
    "# #       y2 = image*noise2\n",
    "# #       image = lintolog(image)\n",
    "# #       y1 = simulate_speckle(image, 10)\n",
    "# #       y2 = simulate_speckle(image, 10)\n",
    "# #       noisy1 = lintolog(noisy1)\n",
    "#       y1 = image + torch.from_numpy(np.random.normal(0, 0.1, image.shape))\n",
    "# #       y2 = image + torch.from_numpy(np.random.normal(0, 0.1, image.shape))\n",
    "# #       y1 = lintolog(y1)\n",
    "# #       y2 = lintolog(y2)\n",
    "# #       w = (y1+y2)/2\n",
    "# #       v = w+torch.DoubleTensor(np.random.normal(0, torch.var(w), size=w.size()))\n",
    "#       if use_cuda:\n",
    "# #         v = v.cuda()\n",
    "# #         w = w.cuda()\n",
    "#         image = image.cuda()\n",
    "#         y1 = y1.cuda()\n",
    "#       y_hat = model(y1)\n",
    "# #       var = 0.0014\n",
    "#       sigma = 5.1\n",
    "#       eps = 1.6*sigma*0.0001\n",
    "#       var = (sigma/255.0)**2\n",
    "#       x_dim = 256\n",
    "#       n = torch.DoubleTensor(np.random.normal(0, 1, size=y1.size()))\n",
    "#       neps = torch.DoubleTensor(n*eps).cuda()\n",
    "#       img_z = y1 + neps\n",
    "#       z_hat = model(img_z)\n",
    "#       loss = criterion(image, y_hat, z_hat, n.cuda(), x_dim, var, 10, eps)\n",
    "#       valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "    \n",
    "    print()   \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss,\n",
    "#         valid_loss\n",
    "        ))\n",
    "    wandb.log({\"Train Loss\": train_loss, \n",
    "#               \"Valid Loss\": valid_loss\n",
    "              })\n",
    "    if train_loss <= train_loss_min:\n",
    "        print('Train loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(train_loss_min, train_loss))\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "#         wandb.save(save_path)\n",
    "        train_loss_min = train_loss\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QfF9wB2Iv9_v",
    "outputId": "0b7b789d-e4fe-42dc-f4fe-63029de10445"
   },
   "outputs": [],
   "source": [
    "model_trained = train_SAR_ESURE_valid(30, trainloaderSAR, validloaderSAR, model_SAR, optimizer, criterion, use_cuda, './checkpoints/checkpoint_SAR.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ExperimentingSSL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07c026139fcf491ab74a455826431249": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46ae165f208047eeb6aef286b7f2de75",
      "placeholder": "​",
      "style": "IPY_MODEL_dba07604ff274ff481ade4ed88102624",
      "value": " 83.3M/83.3M [00:00&lt;00:00, 154MB/s]"
     }
    },
    "3e6a6a7dd0d24cb783f0d8791fc72f97": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46ae165f208047eeb6aef286b7f2de75": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab0a916e20d74319b3e01af720a36503": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf0c992fd9df4347808e8eec92df7c1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc55068d7d3d4a50845c8f4f2d6b96b3",
       "IPY_MODEL_07c026139fcf491ab74a455826431249"
      ],
      "layout": "IPY_MODEL_ab0a916e20d74319b3e01af720a36503"
     }
    },
    "db28821fa4db4e4e8cdf6f256714a350": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "dba07604ff274ff481ade4ed88102624": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc55068d7d3d4a50845c8f4f2d6b96b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e6a6a7dd0d24cb783f0d8791fc72f97",
      "max": 87306240,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_db28821fa4db4e4e8cdf6f256714a350",
      "value": 87306240
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
